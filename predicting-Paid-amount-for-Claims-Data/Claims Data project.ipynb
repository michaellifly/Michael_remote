{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color=blue|red|green|pink|yellow> **DATA 900** </font>\n",
    " \n",
    "## **Claims Data Assignment**\n",
    "\n",
    "<font color=blue|red|green|pink|yellow> **Nemshan ** </font>\n",
    "\n",
    "<img src=http://www.actionforbetterhealthcare.com/wp-content/uploads/2014/11/400-04794931b-590x260.jpg></img>\n",
    "\n",
    "\n",
    "*Medical claims file for 2016 contains ~17 millions rows and ~60 columns of data, containing ~6.5 million individual medical claims. These claims are all commercial claims that were filed by healthcare providers in 2016 in the state of NH. These claims were ~88% for residents of NH and the remaining for out of state visitors who sought care in NH. Each claim consists of one or more line items, each indicating a procedure done during the doctor’s visit. Two columns indicating Billed amount and the Paid amount for the care provided, are of primary interest.*\n",
    "<font color=blue|red|green|pink|yellow> The main objective is to predict “Paid amount per procedure” by mapping a plethora of features available in the dataset </font>\n",
    "\n",
    "**My approach solving this problem was through 3 steps:**\n",
    "\n",
    "### 1. first I used the chunking technique because I had a big file \" 3.73 GB.\" and I chunked that file into small peces to put them into one dataframe.\n",
    "   + I wrote to CSV file so that I can use it later.\n",
    "   + created CSV  file with 1,000,000 sampled claim ids\n",
    "   + joined the data into a subsetted data file\n",
    "### 2. Data cleaning and exploration\n",
    "    \n",
    "    + I removed 51 columns for these reasons : \n",
    "         + 63 features are too many to predict with\n",
    "         + if the column has a lot of missing data.\"more than 2 millon\"\n",
    "         + if I think it would not add predictive value to the model\n",
    "         + I used JMP to look at the distributions of the features and found some of the features had a lot of just one value which seems not useful to be used in predicting. \n",
    "    + I generalized the Icd_code into small numbers of categories based on the starting letters of code because it has a lot of categories\n",
    "    + I thought the client_los would be a good predictor, but it has a lot of missing data. SO I assumed that the missing data were zeros.\n",
    "    + I made indicator variable of gender by changing M  to 1 and F to 0.\n",
    "    + before I removed diagnosis columns, I counted how many diagnoses each claim has and made that into a new column.\n",
    "    + I changed all categorical variables to dummy variables.\n",
    "    + also I removed a few rows that have a missing value. \n",
    "### 3. modling :-\n",
    "    + I started with liner models lasso and ridge. In this data, I thought the liner models would not do well and I didn't expect to see a linear trend. After running lasso and rdge a got a low R^2. \n",
    "    + I used nonlinear models ( MARS, Random forest and Adaboost)\n",
    "\n",
    "        + MARS : \n",
    "        I ran Earth model with different hyper-parameters. I chose the best model with the best R^2 score.\n",
    "\n",
    "        + Random forest :\n",
    "        I tried a lot of different hyper parameter to tune the model. I saw that 300 trees were good enough to run the model without taking a lot of time. But at the end, 44% as the best R^2 score I got which seems not a good model.\n",
    "        + Adaboost :\n",
    "        After trying a different number of estimators for  AdaBoost, the model I got was with 200 estimators. And it seems this modeling algorithm is not the best choice to use for this data. \n",
    "\n",
    "\n",
    "\n",
    "## more explation of each step throuh out the code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunking data with chunk size: 100000\n",
      "Number of chunks to be used: 170 \n",
      "\n",
      "Writing data to file...\n",
      "\n",
      "Length of subsetted df: 16982295 \n",
      "\n",
      "Time elapsed: 17:22 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "folder_path = '/use the directory file path/'\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# open the whole file\n",
    "full_file = open(folder_path + 'PUBLICUSE_CLAIM_MC_2016.txt', 'r')\n",
    "\n",
    "# keep track of how long the process takes\n",
    "start_time = time.time()\n",
    "\n",
    "# change this to try a different number of rows per chunk\n",
    "chunk_rows = 100000\n",
    "\n",
    "num_chunks = math.ceil(16982295/chunk_rows)\n",
    "print('\\nChunking data with chunk size: ', chunk_rows, sep = '')\n",
    "print('Number of chunks to be used:', num_chunks, '\\n')\n",
    "\n",
    "# chunking data using read_csv chunksize parameter\n",
    "small_df = pd.read_csv(full_file, sep = '|', header = 0, chunksize = chunk_rows)\n",
    "\n",
    "# chunks the data into one pandas df\n",
    "full_df = pd.concat(small_df, ignore_index=True)\n",
    "\n",
    "print('Writing data to file...\\n')\n",
    "output_path = '/use the directory file path/'\n",
    "full_df.to_csv(output_path + 'full_data.csv', index = False)\n",
    "print('Length of subsetted df:', len(full_df), '\\n')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "m, s = divmod(elapsed_time, 60)\n",
    "time = \"%02d:%02d\" % (m, s)\n",
    "print('Time elapsed:', time, '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding unique claim IDs...\n",
      "\n",
      "Number of unique claim IDs: 7030309\n",
      "Number of claim IDs sampled: 1000000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# create csv with 1,000,000 sampled claim ids\n",
    "claims_set = set() # create empty set to append unique claims to\n",
    "print('Finding unique claim IDs...\\n')\n",
    "for id_num in full_df.iloc[:, 4]:\n",
    "    if id_num not in claims_set:\n",
    "        claims_set.add(id_num)\n",
    "\n",
    "claims_list = list(claims_set)\n",
    "\n",
    "million_claims_list = random.sample(claims_list, 1000000)\n",
    "million_claims_df = pd.DataFrame(million_claims_list, columns=[\"CLAIM_ID_KEY\"])\n",
    "\n",
    "print('Number of unique claim IDs:', len(claims_list))\n",
    "print('Number of claim IDs sampled:', len(million_claims_list), '\\n')\n",
    "\n",
    "output_path = '/use the directory file path/'\n",
    "million_claims_df.to_csv(output_path + 'one_million_claims.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunk 1 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 2 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 3 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 4 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 5 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 6 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 7 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 8 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 9 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 10 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 11 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 12 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 13 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 14 of 17.\n",
      "Number of Claims: 1000000 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 15 of 17.\n",
      "Number of Claims: 982296 - Number of IDs: 1000000 \n",
      "\n",
      "Finished chunk 16 of 17.\n",
      "Number of Claims: 0 - Number of IDs: 1000000 \n",
      "\n",
      "\n",
      "Writing data to csv...\n",
      "\n",
      "Subsetted data file length: 2131941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# join data into a subsetted data file\n",
    "path = '/use the directory file path/'\n",
    "\n",
    "subset_df = pd.DataFrame()\n",
    "\n",
    "million_claims = pd.read_csv(path + 'one_million_claims.csv', sep = \",\")\n",
    "for i in range(1, 17):\n",
    "    j = (i + 1) * 1000000\n",
    "    lista = list(range(1, j))\n",
    "\n",
    "    claims_file = pd.read_csv(path + 'full_data.csv', sep = ',', nrows = 1000000, skiprows = lista)\n",
    "    mergedata = pd.merge(million_claims, claims_file, how = 'inner', on = ['CLAIM_ID_KEY'])\n",
    "    subset_df = pd.concat([subset_df, mergedata])\n",
    "\n",
    "    print(\"Finished chunk\", i, 'of 17.')\n",
    "    print('Number of Claims:', len(claims_file), '- Number of IDs:', len(million_claims), '\\n')\n",
    "\n",
    "print('\\nWriting data to csv...\\n', sep = '')\n",
    "subset_df.to_csv(path + 'subsetted_data.csv', index = False)\n",
    "print('Subsetted data file length: ', len(subset_df), sep = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
      "Testing subsetted dataframe...\n",
      "Length of subsetted data: 2131941\n",
      "Number of unique claim IDs: 935073\n",
      "Number of columns: 63 \n",
      "\n",
      "\n",
      "Columns with <1,000,000 missing values: \n",
      "CLAIM_ID_KEY: 0\n",
      "COVERAGE_CLASS: 0\n",
      "FROM_YEAR: 0\n",
      "ADM_YR: 0\n",
      "DIS_YR: 0\n",
      "CS_CLAIM_ID_KEY: 0\n",
      "SERVICES_KEY: 0\n",
      "SV_LINE: 0\n",
      "FORM_TYPE: 0\n",
      "SV_STAT: 0\n",
      "POS: 0\n",
      "AGE: 0\n",
      "SEX: 0\n",
      "MEMBER_COUNTY: 0\n",
      "MEMBER_STATE: 0\n",
      "PRODUCT_TYPE: 45867\n",
      "LOB: 0\n",
      "INSURANCE_TYPE: 0\n",
      "PROC_CODE: 154614\n",
      "CPT_MOD1: 0\n",
      "CPT_MOD2: 0\n",
      "ICD_10_OR_HIGHER: 10003\n",
      "ICD_DIAG_01_PRIMARY: 10003\n",
      "ICD_DIAG_02: 940663\n",
      "SERV_PROV_CW_KEY: 51\n",
      "BILL_PROV_CW_KEY: 51\n",
      "QTY: 0\n",
      "AMT_BILLED: 0\n",
      "AMT_PAID: 0\n",
      "AMT_DEDUCT: 0\n",
      "AMT_COINS: 0\n",
      "AMT_COPAY: 0\n",
      "AMT_PREPAID: 0\n",
      "INPATIENT_FLAG: 0\n",
      "MR_LINE_CASE_KEY: 0\n",
      "UTILS: 710770\n",
      "CLAIM_STATUS_ORIG: 0\n",
      "ADMIT_HOUR: 380969\n",
      "DISCHARGE_HOUR: 185706\n",
      "CLAIM_ADJUSTMENT_LOGIC: 0\n",
      "IMPUTED_SERVICE_KEY: 0\n",
      "Num_diag: 0\n",
      "\n",
      "Columns with >1,000,000 missing values: \n",
      "DIS_STAT: 1446012\n",
      "REV_CODE: 1446668\n",
      "UB_BILL_TYPE: 1446475\n",
      "ADM_SRC: 1438185\n",
      "ADM_TYPE: 1447503\n",
      "CLIENT_LOS: 1929869\n",
      "ICD_PROC_01_PRI: 2017881\n",
      "ICD_DIAG_ADMIT: 2037440\n",
      "ICD_DIAG_03: 1352140\n",
      "ICD_DIAG_04: 1588965\n",
      "ICD_DIAG_05: 1824752\n",
      "ICD_DIAG_06: 1904658\n",
      "ICD_DIAG_07: 1951779\n",
      "ICD_DIAG_08: 1986185\n",
      "ICD_DIAG_09: 2013286\n",
      "ICD_DIAG_10: 2040794\n",
      "ICD_DIAG_11: 2056767\n",
      "ICD_DIAG_12: 2069146\n",
      "ICD_DIAG_13: 2083151\n",
      "CASES: 2126757\n",
      "NDC: 2116196\n",
      "ECODE_ORIG: 2080322\n",
      "\n",
      "Columns with >2,000,000 missing values: \n",
      "ICD_PROC_01_PRI: 2017881\n",
      "ICD_DIAG_ADMIT: 2037440\n",
      "ICD_DIAG_09: 2013286\n",
      "ICD_DIAG_10: 2040794\n",
      "ICD_DIAG_11: 2056767\n",
      "ICD_DIAG_12: 2069146\n",
      "ICD_DIAG_13: 2083151\n",
      "CASES: 2126757\n",
      "NDC: 2116196\n",
      "ECODE_ORIG: 2080322\n",
      "\n",
      "New number of columns: 43\n",
      "Columns removed: \n",
      " ['DIS_STAT', 'REV_CODE', 'UB_BILL_TYPE', 'ADM_SRC', 'ADM_TYPE', 'ICD_PROC_01_PRI', 'ICD_DIAG_ADMIT', 'ICD_DIAG_03', 'ICD_DIAG_04', 'ICD_DIAG_05', 'ICD_DIAG_06', 'ICD_DIAG_07', 'ICD_DIAG_08', 'ICD_DIAG_09', 'ICD_DIAG_10', 'ICD_DIAG_11', 'ICD_DIAG_12', 'ICD_DIAG_13', 'CASES', 'NDC', 'ECODE_ORIG'] \n",
      "\n",
      "Columns remaining: \n",
      " ['CLAIM_ID_KEY', 'COVERAGE_CLASS', 'FROM_YEAR', 'ADM_YR', 'DIS_YR', 'CS_CLAIM_ID_KEY', 'SERVICES_KEY', 'SV_LINE', 'FORM_TYPE', 'SV_STAT', 'POS', 'AGE', 'SEX', 'MEMBER_COUNTY', 'MEMBER_STATE', 'PRODUCT_TYPE', 'LOB', 'INSURANCE_TYPE', 'PROC_CODE', 'CPT_MOD1', 'CPT_MOD2', 'UB_BILL_TYPE', 'ADM_TYPE', 'CLIENT_LOS', 'ICD_10_OR_HIGHER', 'ICD_DIAG_01_PRIMARY', 'ICD_DIAG_02', 'ICD_DIAG_04', 'ICD_DIAG_06', 'ICD_DIAG_08', 'ICD_DIAG_10', 'ICD_DIAG_12', 'SERV_PROV_CW_KEY', 'BILL_PROV_CW_KEY', 'QTY', 'AMT_BILLED', 'AMT_PAID', 'AMT_DEDUCT', 'AMT_COINS', 'AMT_COPAY', 'AMT_PREPAID', 'INPATIENT_FLAG', 'MR_LINE_CASE_KEY', 'UTILS', 'CLAIM_STATUS_ORIG', 'ADMIT_HOUR', 'DISCHARGE_HOUR', 'CLAIM_ADJUSTMENT_LOGIC', 'IMPUTED_SERVICE_KEY', 'Num_diag'] \n",
      "\n",
      "Length of subsetted data (again): 2131941\n",
      "Saving data to new csv...\n",
      "\n",
      "Save complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# testing to make sure data looks right\n",
    "print('\\n# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #' , sep = '')\n",
    "print('Testing subsetted dataframe...')\n",
    "data_path = '/use the directory file path/'\n",
    "\n",
    "testing_df = pd.read_csv(data_path + 'subsetted_data.csv', sep = ',')\n",
    "print('Length of subsetted data:', len(testing_df))\n",
    "\n",
    "claims_set_test = set()\n",
    "for id_num in testing_df.loc[:, 'CLAIM_ID_KEY']:\n",
    "    if id_num not in claims_set_test:\n",
    "        claims_set_test.add(id_num)\n",
    "print('Number of unique claim IDs:', len(claims_set_test))\n",
    "print('Number of columns:', len(list(testing_df)), '\\n')\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# data cleaning and prepation and exploration\n",
    "sub_data = testing_df.loc[:, ['CLAIM_ID_KEY', 'ICD_DIAG_01_PRIMARY', 'ICD_DIAG_02', 'ICD_DIAG_03', 'ICD_DIAG_04', 'ICD_DIAG_05', 'ICD_DIAG_06', 'ICD_DIAG_07', 'ICD_DIAG_07', 'ICD_DIAG_08', 'ICD_DIAG_09', 'ICD_DIAG_10', 'ICD_DIAG_11', 'ICD_DIAG_12', 'ICD_DIAG_13']]\n",
    "sub_cols = list(sub_data)\n",
    "sub_data['Num_diag'] = sub_data[sub_cols].count(axis = 1) - 1\n",
    "sub_data1 = sub_data['Num_diag']\n",
    "testing_df = pd.concat([testing_df, sub_data1], axis = 1)\n",
    "\n",
    "# print(testing_df.isnull().sum() > 1000000)\n",
    "missing_vals = testing_df.isnull().sum()\n",
    "cols = []\n",
    "rm_cols_list = []\n",
    "\n",
    "# make list of all columns\n",
    "for index, val in enumerate(missing_vals):\n",
    "    cols.append(testing_df.columns[index])\n",
    "\n",
    "print('\\nColumns with <1,000,000 missing values: ', sep = '')\n",
    "for index, val in enumerate(missing_vals):\n",
    "    if val < 1000000:\n",
    "        print(testing_df.columns[index], ': ', val, sep = '')\n",
    "\n",
    "print('\\nColumns with >1,000,000 missing values: ', sep = '')\n",
    "for index, val in enumerate(missing_vals):\n",
    "    if val > 1000000:\n",
    "        print(testing_df.columns[index], ': ', val, sep = '')\n",
    "        rm_cols_list.append(testing_df.columns[index])\n",
    "\n",
    "print('\\nColumns with >2,000,000 missing values: ', sep = '')\n",
    "for index, val in enumerate(missing_vals):\n",
    "    if val > 2000000:\n",
    "        print(testing_df.columns[index], ': ', val, sep = '')\n",
    "\n",
    "rm_cols_list.remove('CLIENT_LOS')\n",
    "\n",
    "testing_df = testing_df.drop(rm_cols_list,axis =1)\n",
    "\n",
    "for item in cols:\n",
    "    if item in rm_cols_list:\n",
    "        cols.remove(item)\n",
    "\n",
    "print('\\nNew number of columns: ', len(list(testing_df)), sep = '')\n",
    "print('Columns removed:', '\\n', rm_cols_list, '\\n')\n",
    "print('Columns remaining:', '\\n', cols, '\\n')\n",
    "print('Length of subsetted data (again):', len(testing_df))\n",
    "\n",
    "# save to csv for more cleaning and faster loading\n",
    "print('Saving data to new csv...\\n')\n",
    "testing_df.to_csv(data_path + 'data_cleaning_step1.csv', index = False)\n",
    "print('Save complete.\\n')\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting number of columns: 43\n",
      "Number of columns remaining: 23 \n",
      "\n",
      "Data types: \n",
      " CLAIM_ID_KEY             int64\n",
      "COVERAGE_CLASS          object\n",
      "CS_CLAIM_ID_KEY        float64\n",
      "FORM_TYPE               object\n",
      "SV_STAT                 object\n",
      "POS                     object\n",
      "AGE                     object\n",
      "SEX                     object\n",
      "MEMBER_COUNTY          float64\n",
      "PRODUCT_TYPE            object\n",
      "LOB                     object\n",
      "PROC_CODE               object\n",
      "CLIENT_LOS             float64\n",
      "ICD_DIAG_01_PRIMARY     object\n",
      "ICD_DIAG_02             object\n",
      "SERV_PROV_CW_KEY       float64\n",
      "BILL_PROV_CW_KEY       float64\n",
      "AMT_BILLED             float64\n",
      "AMT_PAID               float64\n",
      "AMT_DEDUCT             float64\n",
      "AMT_COINS              float64\n",
      "MR_LINE_CASE_KEY       float64\n",
      "Num_diag                 int64\n",
      "dtype: object \n",
      "\n",
      "Changing SEX to Gender_Code and dropping original column...\n",
      "\n",
      "Changing PROC_CODE to Proc_Code_letters and dropping original column...\n",
      "\n",
      "Changing AGE to Age and dropping original column...\n",
      "\n",
      "Adding zeros for missing values in CLIENT_LOS...\n",
      "\n",
      "Number of columns remaining: 23 \n",
      "\n",
      "Data types:\n",
      "CLAIM_ID_KEY             int64\n",
      "COVERAGE_CLASS          object\n",
      "CS_CLAIM_ID_KEY        float64\n",
      "FORM_TYPE               object\n",
      "SV_STAT                 object\n",
      "POS                     object\n",
      "MEMBER_COUNTY          float64\n",
      "PRODUCT_TYPE            object\n",
      "LOB                     object\n",
      "CLIENT_LOS             float64\n",
      "ICD_DIAG_01_PRIMARY     object\n",
      "ICD_DIAG_02             object\n",
      "SERV_PROV_CW_KEY       float64\n",
      "BILL_PROV_CW_KEY       float64\n",
      "AMT_BILLED             float64\n",
      "AMT_PAID               float64\n",
      "AMT_DEDUCT             float64\n",
      "AMT_COINS              float64\n",
      "MR_LINE_CASE_KEY       float64\n",
      "Num_diag                 int64\n",
      "Gender_Code              int64\n",
      "Proc_Code_letters       object\n",
      "Age                      int64\n",
      "dtype: object\n",
      "\n",
      "Writing new csv...\n",
      "Missing values per column:\n",
      "CLAIM_ID_KEY                0\n",
      "COVERAGE_CLASS              0\n",
      "CS_CLAIM_ID_KEY             0\n",
      "FORM_TYPE                   0\n",
      "SV_STAT                     0\n",
      "POS                         0\n",
      "MEMBER_COUNTY               0\n",
      "PRODUCT_TYPE            45867\n",
      "LOB                         0\n",
      "CLIENT_LOS                  0\n",
      "ICD_DIAG_01_PRIMARY     10003\n",
      "ICD_DIAG_02            940663\n",
      "SERV_PROV_CW_KEY           51\n",
      "BILL_PROV_CW_KEY           51\n",
      "AMT_BILLED                  0\n",
      "AMT_PAID                    0\n",
      "AMT_DEDUCT                  0\n",
      "AMT_COINS                   0\n",
      "MR_LINE_CASE_KEY            0\n",
      "Num_diag                    0\n",
      "Gender_Code                 0\n",
      "Proc_Code_letters           0\n",
      "Age                         0\n",
      "dtype: int64\n",
      "\n",
      "Are BILL_PROV_CW_KEY and SERV_PROV_CW_KEY equal?\n",
      "\n",
      "True     1702918\n",
      "False     429023\n",
      "dtype: int64\n",
      "Are CLAIM_ID_KEY and CS_CLAIM_ID_KEY equal?\n",
      "\n",
      "True     1986035\n",
      "False     145906\n",
      "dtype: int64\n",
      "Dropping ICD_DIAG_01_PRIMARY column...\n",
      "\n",
      "Changing ICD_DIAG_01_PRIMARY column...\n",
      "\n",
      "Changing ICD_DIAG_01_PRIMARY to ICD_DIAG_01_PRIMARY_categories and dropping original column...\n",
      "\n",
      "Are ICD_DIAG_01_PRIMARY_categories and Proc_Code_letters equal?\n",
      "\n",
      "False    2125083\n",
      "True        6858\n",
      "dtype: int64\n",
      "Dropping all rows with null values...\n",
      "\n",
      "Dropping rows with negative bill and paid amounts...\n",
      "\n",
      "Missing values per column:\n",
      "CLAIM_ID_KEY                      0\n",
      "COVERAGE_CLASS                    0\n",
      "FORM_TYPE                         0\n",
      "SV_STAT                           0\n",
      "POS                               0\n",
      "MEMBER_COUNTY                     0\n",
      "PRODUCT_TYPE                      0\n",
      "LOB                               0\n",
      "CLIENT_LOS                        0\n",
      "SERV_PROV_CW_KEY                  0\n",
      "AMT_BILLED                        0\n",
      "AMT_PAID                          0\n",
      "AMT_DEDUCT                        0\n",
      "AMT_COINS                         0\n",
      "MR_LINE_CASE_KEY                  0\n",
      "Num_diag                          0\n",
      "Gender_Code                       0\n",
      "Proc_Code_letters                 0\n",
      "Age                               0\n",
      "ICD_DIAG_01_PRIMARY_categories    0\n",
      "dtype: int64\n",
      "\n",
      "Number of columns remaining: 20 \n",
      "\n",
      "Writing new csv...\n",
      "Opening data file...\n",
      "\n",
      "Dropping columns...\n",
      "\n",
      "Dropping rows with negative bill and paid amounts...\n",
      "\n",
      "Missing values per column:\n",
      "FORM_TYPE                         0\n",
      "SV_STAT                           0\n",
      "PRODUCT_TYPE                      0\n",
      "CLIENT_LOS                        0\n",
      "AMT_BILLED                        0\n",
      "AMT_PAID                          0\n",
      "AMT_DEDUCT                        0\n",
      "AMT_COINS                         0\n",
      "Num_diag                          0\n",
      "Gender_Code                       0\n",
      "Age                               0\n",
      "ICD_DIAG_01_PRIMARY_categories    0\n",
      "dtype: int64\n",
      "\n",
      "Number of columns remaining: 12\n",
      "Length of data: 2013373 \n",
      "\n",
      "Merging some ICD categories...\n",
      "\n",
      "Writing new csv...\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "data_path = '/use the directory file path/'\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# further cleaning of data\n",
    "# start_time = time.time()\n",
    "data = pd.read_csv(data_path + 'data_cleaning_step1.csv', sep = ',')\n",
    "print('\\nStarting number of columns: ', len(list(data)), sep = '')\n",
    "\n",
    "# removing columns decided ahead of time\n",
    "cols_to_remove = ['FROM_YEAR', 'ADM_YR', 'DIS_YR', 'SERVICES_KEY', 'SV_LINE', 'MEMBER_STATE', 'INSURANCE_TYPE', 'CPT_MOD1', 'CPT_MOD2', 'ICD_10_OR_HIGHER', 'QTY', 'AMT_COPAY', 'AMT_PREPAID', 'INPATIENT_FLAG', 'UTILS', 'CLAIM_STATUS_ORIG', 'ADMIT_HOUR', 'DISCHARGE_HOUR', 'CLAIM_ADJUSTMENT_LOGIC', 'IMPUTED_SERVICE_KEY']\n",
    "\n",
    "data = data.drop(cols_to_remove,axis =1)\n",
    "print('Number of columns remaining:', len(list(data)), '\\n')\n",
    "print('Data types:', '\\n', data.dtypes, '\\n')\n",
    "\n",
    "# make age numeric , make gender 1 or 0, proc code to categories of letters, pos to dummies \n",
    "\n",
    "# gender\n",
    "print('Changing SEX to Gender_Code and dropping original column...\\n')\n",
    "data['Gender_Code'] = data['SEX'].apply(lambda x: 1 if x == 'M' else 0)\n",
    "data = data.drop(['SEX'], axis=1)\n",
    "\n",
    "# proc code into letter categories\n",
    "print('Changing PROC_CODE to Proc_Code_letters and dropping original column...\\n')\n",
    "data['Proc_Code_letters'] = data['PROC_CODE'].apply(lambda x: str(x)[0])\n",
    "data = data.drop(['PROC_CODE'], axis=1)\n",
    "\n",
    "# getting rid of categorical age - making 90+ into 90\n",
    "print('Changing AGE to Age and dropping original column...\\n')\n",
    "data['Age'] = data['AGE'].apply(lambda x: 90 if x == '90+' else x)\n",
    "data['Age'] = data['Age'].astype(str).astype(int)\n",
    "data = data.drop( ['AGE'], axis=1)\n",
    "\n",
    "# add zeros to missing CLIENT_LOS\n",
    "print('Adding zeros for missing values in CLIENT_LOS...\\n')\n",
    "data['CLIENT_LOS'] = data['CLIENT_LOS'].apply(lambda x: 0 if pd.isnull(x) else x)\n",
    "\n",
    "# summary after cleaning\n",
    "print('Number of columns remaining:', len(list(data)), '\\n')\n",
    "print('Data types:', '\\n', data.dtypes, '\\n', sep = '')\n",
    "\n",
    "# output to csv again ... smaller this time\n",
    "print('Writing new csv...')\n",
    "data.to_csv(data_path + 'clean_step_3.csv', index = False)\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# m, s = divmod(elapsed_time, 60)\n",
    "# time = \"%02d:%02d\" % (m, s)\n",
    "# print('\\nTime elapsed: ', time, '\\n', sep = '')\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# cleaning from cleaner file\n",
    "# start_time = time.time()\n",
    "#print('Opening data file...\\n')\n",
    "data2 = pd.read_csv(data_path + 'clean_step_3.csv', sep = ',')\n",
    "\n",
    "# check missing values for each column\n",
    "missing_vals = data2.isnull().sum()\n",
    "print('Missing values per column:\\n', missing_vals, '\\n', sep = '')\n",
    "\n",
    "# test if these two columns are equal\n",
    "print('Are BILL_PROV_CW_KEY and SERV_PROV_CW_KEY equal?\\n')\n",
    "print((data2['SERV_PROV_CW_KEY'] == data2['BILL_PROV_CW_KEY']).value_counts())\n",
    "# neither actually seem too helpful anyways - drop one for now - come back for the other?\n",
    "data2 = data2.drop(['BILL_PROV_CW_KEY'], axis=1)\n",
    "\n",
    "# test if these two columns are equal\n",
    "print('Are CLAIM_ID_KEY and CS_CLAIM_ID_KEY equal?\\n')\n",
    "print((data2['CLAIM_ID_KEY'] == data2['CS_CLAIM_ID_KEY']).value_counts())\n",
    "data2 = data2.drop(['CS_CLAIM_ID_KEY'], axis=1)\n",
    "\n",
    "\n",
    "# ICD_DIAG_02 has almost half missing values\n",
    "print('Dropping ICD_DIAG_01_PRIMARY column...\\n')\n",
    "data2 = data2.drop(['ICD_DIAG_02'], axis=1)\n",
    "\n",
    "# change ICD_DIAG_01_PRIMARY to just first letters\n",
    "print('Changing ICD_DIAG_01_PRIMARY column...\\n')\n",
    "print('Changing ICD_DIAG_01_PRIMARY to ICD_DIAG_01_PRIMARY_categories and dropping original column...\\n')\n",
    "data2['ICD_DIAG_01_PRIMARY_categories'] = data2['ICD_DIAG_01_PRIMARY'].apply(lambda x: str(x)[0])\n",
    "data2 = data2.drop(['ICD_DIAG_01_PRIMARY'], axis=1)\n",
    "\n",
    "\n",
    "# test if these two columns are equal\n",
    "print('Are ICD_DIAG_01_PRIMARY_categories and Proc_Code_letters equal?\\n')\n",
    "print((data2['ICD_DIAG_01_PRIMARY_categories'] == data2['Proc_Code_letters']).value_counts())\n",
    "\n",
    "\n",
    "# drop all rows with null values\n",
    "print('Dropping all rows with null values...\\n')\n",
    "data2 = data2.dropna(axis = 0, how = 'any')\n",
    "\n",
    "# drop all rows with negaitive billing amounts and paid amounts\n",
    "print('Dropping rows with negative bill and paid amounts...\\n')\n",
    "data2 = data2[data2['AMT_BILLED'] >= 0]\n",
    "data2 = data2[data2['AMT_PAID'] >= 0]\n",
    "\n",
    "\n",
    "# check missing values for each column before writing to csv\n",
    "missing_vals = data2.isnull().sum()\n",
    "print('Missing values per column:\\n', missing_vals, '\\n', sep = '')\n",
    "print('Number of columns remaining:', len(list(data2)), '\\n')\n",
    "\n",
    "# output to csv again\n",
    "print('Writing new csv...')\n",
    "data2.to_csv(data_path + 'clean_step_4.csv', index = False)\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# m, s = divmod(elapsed_time, 60)\n",
    "# time = \"%02d:%02d\" % (m, s)\n",
    "# print('\\nTime elapsed: ', time, '\\n', sep = '')\n",
    "\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# cleaning more - dropping columns\n",
    "# start_time = time.time()\n",
    "print('Opening data file...\\n')\n",
    "data3 = pd.read_csv(data_path + 'clean_step_4.csv', sep = ',')\n",
    "\n",
    "# drop CLAIM_ID_KEY, MEMBER_COUNTY, SERV_PROV_CW_KEY, MR_LINE_CASE_KEY, Proc_Code_letters, COVERAGE_CLASS, LOB\n",
    "print('Dropping columns...\\n')\n",
    "data3 = data3.drop( ['CLAIM_ID_KEY'],axis=1)\n",
    "data3 = data3.drop(['MEMBER_COUNTY'],axis=1)\n",
    "data3 = data3.drop(['SERV_PROV_CW_KEY'],axis=1)\n",
    "data3 = data3.drop( ['MR_LINE_CASE_KEY'],axis=1)\n",
    "data3 = data3.drop( ['Proc_Code_letters'],axis=1)\n",
    "data3 = data3.drop( ['COVERAGE_CLASS'],axis=1)\n",
    "data3 = data3.drop( ['LOB'],axis=1)\n",
    "data3 = data3.drop( ['POS'],axis=1)\n",
    "\n",
    "# drop negative coins and deduct amounts\n",
    "print('Dropping rows with negative bill and paid amounts...\\n')\n",
    "data3 = data3[data3['AMT_COINS'] >= 0]\n",
    "data3 = data3[data3['AMT_DEDUCT'] >= 0]\n",
    "\n",
    "missing_vals = data3.isnull().sum()\n",
    "print('Missing values per column:\\n', missing_vals, '\\n', sep = '')\n",
    "print('Number of columns remaining:', len(list(data3)))\n",
    "print('Length of data:', len(data3), '\\n')\n",
    "\n",
    "\n",
    "# combine some of the icd10 codes?\n",
    "print('Merging some ICD categories...\\n')\n",
    "data3['ICD_DIAG_01_PRIMARY_categories'] = data3['ICD_DIAG_01_PRIMARY_categories'].apply(lambda x: 'V' if x == 'W' else x)\n",
    "data3['ICD_DIAG_01_PRIMARY_categories'] = data3['ICD_DIAG_01_PRIMARY_categories'].apply(lambda x: 'V' if x == 'X' else x)\n",
    "data3['ICD_DIAG_01_PRIMARY_categories'] = data3['ICD_DIAG_01_PRIMARY_categories'].apply(lambda x: 'V' if x == 'Y' else x)\n",
    "data3['ICD_DIAG_01_PRIMARY_categories'] = data3['ICD_DIAG_01_PRIMARY_categories'].apply(lambda x: 'A' if x == 'B' else x)\n",
    "\n",
    "data3['ICD_DIAG_01_PRIMARY_categories'] = data3['ICD_DIAG_01_PRIMARY_categories'].apply(lambda x: 'num' if x.isdigit() else x)\n",
    "\n",
    "data3['ICD_DIAG_01_PRIMARY_categories'] = data3['ICD_DIAG_01_PRIMARY_categories'].apply(lambda x: 'N' if x == 'n' else x)\n",
    "\n",
    "\n",
    "# output to csv again\n",
    "print('Writing new csv...')\n",
    "data3.to_csv(data_path + 'final_clean_data2.csv', index = False)\n",
    "print('Save complete.')\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# m, s = divmod(elapsed_time, 60)\n",
    "# time = \"%02d:%02d\" % (m, s)\n",
    "# print('\\nTime elapsed: ', time, '\\n', sep = '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening data file...\n",
      "Data open.\n",
      "Starting number of columns: 12\n",
      "New number of columns: 42 \n",
      "\n",
      "Splitting data...\n",
      "\n",
      "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
      "\n",
      "Running Lasso model...\n",
      "Lasso r^2 on test data : 0.122718 \n",
      "\n",
      "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
      "\n",
      "Running Ridge model...\n",
      "Ridge r^2 on test data : 0.135109 \n",
      "\n",
      "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
      "\n",
      "\n",
      "Time elapsed: 00:14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modeling step:\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print('Opening data file...')\n",
    "data_path = '/use the directory file path/'\n",
    "df = pd.read_csv(data_path + 'final_clean_data2.csv', index_col = False)\n",
    "print('Data open.')\n",
    "print('Starting number of columns:', len(list(df)))\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# make dummies and normalize data\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df_dummies = pd.get_dummies(df, columns=['FORM_TYPE', 'SV_STAT', 'PRODUCT_TYPE', 'ICD_DIAG_01_PRIMARY_categories'])\n",
    "\n",
    "unstandardized_df = df_dummies\n",
    "\n",
    "# Standardize\n",
    "cols = ['CLIENT_LOS', 'AMT_BILLED','AMT_PAID', 'AMT_DEDUCT',\n",
    "         'AMT_COINS', 'Num_diag', 'Age']\n",
    "\n",
    "normalized_df = df_dummies[cols].apply(zscore)\n",
    "for col in cols:\n",
    "    df_dummies = df_dummies.drop( [col],axis=1)\n",
    "\n",
    "normalized_df = pd.concat([df_dummies, normalized_df], axis = 1)\n",
    "\n",
    "print('New number of columns:', len(list(normalized_df)), '\\n')\n",
    "\n",
    "# print('Columns used:')\n",
    "# for item in list(normalized_df):\n",
    "#     print(item)\n",
    "# print('\\n')\n",
    "\n",
    "df = normalized_df\n",
    "\n",
    "# print('df columns:\\n', list(df), sep = '')\n",
    "# print(len(list(df)))\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Splitting data...\\n')\n",
    "X = normalized_df.loc[:, normalized_df.columns != 'AMT_PAID']\n",
    "y = normalized_df.loc[:, 'AMT_PAID']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 111)\n",
    "# print('X_train length:', len(X_train), 'X_test length:', len(X_test))\n",
    "# print('y_train length:', len(y_train), 'y_test length:', len(y_test))\n",
    "print('# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\\n')\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# LASSO model\n",
    "print('Running Lasso model...')\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alpha_value_lasso = 0.1\n",
    "lasso = Lasso(alpha = alpha_value_lasso)\n",
    "\n",
    "y_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_lasso = r2_score(y_test, y_pred_lasso)\n",
    "print(\"Lasso r^2 on test data : %f\" % r2_score_lasso, '\\n')\n",
    "print('# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\\n')\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# Ridge model\n",
    "print('Running Ridge model...')\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alpha_value_ridge = 0.5\n",
    "ridge = Ridge(alpha = alpha_value_ridge)\n",
    "\n",
    "y_pred_ridge = ridge.fit(X_train, y_train).predict(X_test)\n",
    "r2_score_ridge = r2_score(y_test, y_pred_ridge)\n",
    "print(\"Ridge r^2 on test data : %f\" % r2_score_ridge, '\\n')\n",
    "print('# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\\n')\n",
    "\n",
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "elapsed_time = time.time() - start_time\n",
    "m, s = divmod(elapsed_time, 60)\n",
    "time = \"%02d:%02d\" % (m, s)\n",
    "print('\\nTime elapsed: ', time, '\\n', sep = '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning forward pass\n",
      "---------------------------------------------------------------\n",
      "iter  parent  var  knot  mse       terms  gcv    rsq    grsq   \n",
      "---------------------------------------------------------------\n",
      "0     -       -    -     1.018349  1      1.018  0.000  0.000  \n",
      "1     0       1    52706  0.742267  3      0.742  0.271  0.271  \n",
      "2     1       9    -1    0.582815  4      0.583  0.428  0.428  \n",
      "3     1       6    -1    0.545224  5      0.545  0.465  0.465  \n",
      "4     1       18   -1    0.536123  6      0.536  0.474  0.474  \n",
      "5     1       7    -1    0.530261  7      0.530  0.479  0.479  \n",
      "6     1       37   -1    0.525039  8      0.525  0.484  0.484  \n",
      "7     0       3    1576541  0.520280  10     0.520  0.489  0.489  \n",
      "------------------------------------------------------------------\n",
      "Stopping Condition 2: Improvement below threshold\n",
      "Beginning pruning pass\n",
      "--------------------------------------------\n",
      "iter  bf  terms  mse   gcv    rsq    grsq   \n",
      "--------------------------------------------\n",
      "0     -   10     0.52  0.520  0.489  0.489  \n",
      "1     9   9      0.52  0.520  0.489  0.489  \n",
      "2     8   8      0.53  0.525  0.484  0.484  \n",
      "3     7   7      0.53  0.530  0.479  0.479  \n",
      "4     6   6      0.54  0.536  0.474  0.474  \n",
      "5     5   5      0.55  0.545  0.465  0.465  \n",
      "6     4   4      0.58  0.583  0.428  0.428  \n",
      "7     2   3      0.63  0.627  0.385  0.385  \n",
      "8     3   2      0.79  0.795  0.220  0.220  \n",
      "9     1   1      1.02  1.018  -0.000  -0.000  \n",
      "----------------------------------------------\n",
      "Selected iteration: 0\n",
      "Earth Score: 0.295359\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# Earth model\n",
    "from pyearth import Earth\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "thresh_val = [0.01]\n",
    "scores = []\n",
    "for item in thresh_val:\n",
    "    print('Running Earth model with thresh ', item, '...', sep = '')\n",
    "    earth_model = Earth(max_degree = 3, use_fast = True, smooth = True, thresh = item, verbose = 2)\n",
    "    earth_model.fit(X_train, y_train)\n",
    "\n",
    "    print('Earth Score: %f' % r2_score(y_test, earth_model.predict(X_test)), '\\n')\n",
    "    scores.append(r2_score)\n",
    "    print('# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\\n')\n",
    "\n",
    "print('Threshold values:', thresh_val)\n",
    "print('Scores:', scores)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length: 1610698 X_test length: 402675\n",
      "y_train length: 1610698 y_test length: 402675\n",
      "Running random forest model with optimal settings...\n",
      "Random Forest score: 0.436817204208 \n",
      "\n",
      "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\n",
    "# Random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# re-split data for unstandardized data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# print('Splitting data...')\n",
    "X = unstandardized_df.loc[:, unstandardized_df.columns != 'AMT_PAID']\n",
    "y = unstandardized_df.loc[:, 'AMT_PAID']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 111)\n",
    "print('X_train length:', len(X_train), 'X_test length:', len(X_test))\n",
    "print('y_train length:', len(y_train), 'y_test length:', len(y_test))\n",
    "\n",
    "#modling using the best values to choose whcih one to use\n",
    "# depths = [30, 35, 40, 45, 50, 55]\n",
    "# rf_scores = []\n",
    "# for item in depths:\n",
    "#     print('Running random forest model with max_depth = ', item, '...', sep = '')\n",
    "#     regressor = RandomForestRegressor(n_estimators = 20, max_depth = item, max_features = 'sqrt', n_jobs = 4, oob_score = True, random_state = 1234) #max_depth = 35\n",
    "#     regressor.fit(X_train, y_train)\n",
    "\n",
    "#     predicted = regressor.predict(X_test)\n",
    "#     r2 = r2_score(y_test, predicted)\n",
    "#     print('Random Forest score:', r2, '\\n')\n",
    "#     rf_scores.append(r2)\n",
    "#     print('# --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- #\\n')\n",
    "# for item in depths:\n",
    "#     print(item, ':', rf_scores[depths.index(item)])\n",
    "\n",
    "### BEST MODEL: max feaures = sqrt, max_depth = 30-50\n",
    "print('Running random forest model with optimal settings...', sep = '')\n",
    "regressor = RandomForestRegressor(n_estimators = 300, max_depth = 30, max_features = 'sqrt', n_jobs = -1, oob_score = True) #max_depth = 35\n",
    "regressor.fit(X_train, y_train)\n",
    "predicted = regressor.predict(X_test)\n",
    "r2 = r2_score(y_test, predicted)\n",
    "print('Random Forest score:', r2, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ADA boosted model...\n",
      "ADA boosted regression model score: 0.227440371675 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ada boosted model\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('Running ADA boosted model...')\n",
    "ada_boost = AdaBoostRegressor(n_estimators = 200)\n",
    "ada_boost.fit(X_train, y_train)\n",
    "prediction = ada_boost.predict(X_test)\n",
    "r2 = r2_score(y_test, prediction)\n",
    "# score = ada_boost.score(X_test, )\n",
    "print('ADA boosted regression model score:', r2, '\\n')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaways from working on this project :\n",
    "* I learned how to chunk big files.\n",
    "* I become more familiar with making the data into granule level.\n",
    "* I could have missed features that important in predicting  Paid amount per procedure because of my limited knowledge of healthcare system. \n",
    "* I learned more how to apply some of new the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
